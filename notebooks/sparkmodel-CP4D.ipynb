{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict heart failure with Watson Machine Learning\n",
    "\n",
    "This notebook contains steps and code to create a predictive model to predict heart failure and then deploy that model to Watson Machine Learning so it can be used in an application.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "* Load a CSV file into the Object Storage service linked to your Watson Studio\n",
    "* Create an Apache Spark machine learning model\n",
    "* Train and evaluate a model\n",
    "* Persist a model in a Watson Machine Learning repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "* Create a Deployment Space(Watson Machine Learning service) instance and associate it with your project\n",
    "* Upload heart failure data to the Watson Studio project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a few libraries for this exercise:\n",
    "\n",
    "1. [Watson Machine Learning Client](http://wml-api-pyclient.mybluemix.net/): Client library to work with the Watson Machine Learning service on IBM Cloud.\n",
    "1. [Pixiedust](https://github.com/pixiedust/pixiedust): Python Helper library for Jupyter Notebooks\n",
    "1. [ibmos2spark](https://github.com/ibm-watson-data-lab/ibmos2spark): Facilitates Data I/O between Spark and IBM Object Storage services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user watson-machine-learning-client --upgrade | tail -n 1\n",
    "!pip install --user pyspark==2.3.3 --upgrade|tail -n 1\n",
    "!pip install --upgrade pixiedust | tail -n 1\n",
    "\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Load and Explore Data\n",
    "\n",
    "You'll load our data as a pandas data frame. The dataset should have been uploaded into your Watson Studio project. Otherwise, refer to the repo README file to upload the sample dataset.\n",
    "\n",
    "* Highlight the cell below by clicking it.\n",
    "* Click the `10/01` \"Find data\" icon in the upper right of the notebook.\n",
    "* In the `Find and add data` right hand panel, your data file should be listed.\n",
    "* Click the `Insert to code` drop down menue under your file name.\n",
    "* Select `Insert SparkSession DataFrame`\n",
    "* The code that brings the data into the notebook environment and creates a Pandas DataFrame will be added to the cell below.\n",
    "* Run the cell\n",
    "\n",
    "> **IMPORTANT**: Ensure the DataFrame is named `df_data_1`. If not, rename it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place cursor below and insert the Pandas DataFrame for the sample dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Pandas naming convention df for our DataFrame. Make sure that the cell below uses the name for the dataframe used above. For the locally uploaded file it should look like df_data_1 or df_data_2 or df_data_x. For the virtualized data case it should look like data_df_1 or data_df_2 or data_df_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for virtualized data\n",
    "# df = data_df_1\n",
    "\n",
    "# for local upload\n",
    "df = df_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Drop PATIENTID feature (column)\n",
    "Sometimes, you may want to build your model based on part of your dataset. For example, `PATIENTID` can be irrelevant and you want to remove it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('PATIENTID', axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Explore sample dataset\n",
    "\n",
    "Explore the loaded dataset by using the following DataFrame methods:\n",
    "\n",
    "df.info() to print the data schema\n",
    "df.count() to count all records\n",
    "\n",
    "After removing PATIENTID  column, the dataset contains ten fields and 10800 records. The HEARTFAILURE field is the one we would like to predict (label). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.3 Any NaN values should be removed to create a more accurate model.\n",
    "To check if your dataset contains NaN values and remove NaN values if they are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any NaN values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN values were identified in the sample dataset. The previous command also identified NaN value in the attribute `CHOLESTEROL`.\n",
    "Set `nan_column` to the column number for CHOLESTEROL (starting at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_column = df.columns.get_loc(\"CHOLESTEROL\")\n",
    "print(nan_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for nan_column (TotalCharges)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "\n",
    "df.iloc[:, nan_column] = imp.fit_transform(df.iloc[:, nan_column].values.reshape(-1, 1))\n",
    "df.iloc[:, nan_column] = pd.Series(df.iloc[:, nan_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any NaN values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, NaN values were removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize data\n",
    "\n",
    "Python provides rich set of visualization libraries. For example, with PixieDust's `display()` method you can visually explore the loaded data using built-in charts, such as, bar charts, line charts, scatter plots, or maps.\n",
    "\n",
    "To explore a data set: choose the desired chart type from the drop down, configure chart options, configure display options.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, svm\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, StandardScaler\n",
    "import sklearn.feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot HEARTFAILURE Frequency count\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_palette(\"hls\", 3)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.countplot(x=\"CHOLESTEROL\", hue=\"HEARTFAILURE\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot HEARTFAILURE Frequency count\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_palette(\"hls\", 3)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.countplot(x=\"FAMILYHISTORY\", hue=\"HEARTFAILURE\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot HEARTFAILURE Frequency count\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_palette(\"hls\", 3)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.countplot(x=\"SMOKERLAST5YRS\", hue=\"HEARTFAILURE\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "78",
      "handlerId": "barChart",
      "keyFields": "AGE",
      "kind": "kde",
      "mpld3": "false",
      "rendererId": "seaborn",
      "rowCount": "500",
      "title": "Explore",
      "valueFields": "BMI"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Optional] Another option to explore data is to use the built in data refinery tool. From your main project page in Watson Studio, select the Assets tab and click on the name of your training data. From the data preview page, you can click on the Refine button to load the data into Data Refinery.\n",
    "\n",
    "Understand the quality and distribution of your data using data profiler, and dozens of built-in charts, graphs, and statistics. Automatically detect data types and column classifications. Explore the data, selecting the Profile tab to better understand the values for the columns or features used later when building the machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Create Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data = spark.createDataFrame(df)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Split the data into training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will split your data into: train and test data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = df_data.randomSplit([0.8, 0.20], 24)\n",
    "train_data = split_data[0]\n",
    "test_data = split_data[1]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our data has been successfully split into two data sets:\n",
    "\n",
    "- The train data set, which is the largest group, is used for training.\n",
    "- The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Convert all String Fields to Numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all the string fields to numeric ones by using the StringIndexer transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer_label = StringIndexer(inputCol=\"HEARTFAILURE\", outputCol=\"label\").fit(df_data)\n",
    "stringIndexer_sex = StringIndexer(inputCol=\"SEX\", outputCol=\"SEX_IX\")\n",
    "stringIndexer_famhist = StringIndexer(inputCol=\"FAMILYHISTORY\", outputCol=\"FAMILYHISTORY_IX\")\n",
    "stringIndexer_smoker = StringIndexer(inputCol=\"SMOKERLAST5YRS\", outputCol=\"SMOKERLAST5YRS_IX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create a single vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, create a feature vector by combining all features together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(inputCols=[\"AVGHEARTBEATSPERMIN\",\"PALPITATIONSPERDAY\",\"CHOLESTEROL\",\"BMI\",\"AGE\",\"SEX_IX\",\"FAMILYHISTORY_IX\",\"SMOKERLAST5YRS_IX\",\"EXERCISEMINPERWEEK\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Define Estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define estimators you want to use for classification. Random Forest is used in the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Map Indexed Labels back to Original Labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, indexed labels back to original labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_label.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_df_pipeline = Pipeline(stages=[stringIndexer_label, stringIndexer_sex, stringIndexer_famhist, stringIndexer_smoker, vectorAssembler_features])\n",
    "transformed_df = transform_df_pipeline.fit(df_data).transform(df_data)\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Create Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the pipeline now. A pipeline consists of transformers and an estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[stringIndexer_label, stringIndexer_sex, stringIndexer_famhist, stringIndexer_smoker, vectorAssembler_features, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Train Model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your Random Forest model by using the previously defined pipeline and training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Check Model Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your model accuracy now. To evaluate the model, use test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tune your model now to achieve better accuracy. For simplicity of this example tuning section is omitted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Save the model and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a unique name for MODEL_NAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"my-model-today\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Save the model to ICP4D local Watson Machine Learning\n",
    "Replace the `username` and `password` values of `*****` with your Cloud Pak for Data `username` and `password`.\n",
    "The value for `url` should match the `url` for your Cloud Pak for Data cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "wml_credentials = {\n",
    "                   \"url\": \"https://zen-cpd-zen.apps.os-workshop-nov22.vz-cpd-nov22.com\",\n",
    "                   \"username\": \"*****\",\n",
    "                   \"password\" : \"*****\",\n",
    "                   \"instance_id\": \"wml_local\",\n",
    "                   \"version\" : \"2.5.0\"\n",
    "                  }\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.spaces.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the desired space as the `default_space`\n",
    "Put the `GUID` of the desired space as the parameter below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set.default_space('<GUID>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our model\n",
    "model_props = {client.repository.ModelMetaNames.NAME: MODEL_NAME,\n",
    "               client.repository.ModelMetaNames.RUNTIME_UID : \"spark-mllib_2.3\",\n",
    "               client.repository.ModelMetaNames.TYPE : \"mllib_2.3\"}\n",
    "published_model = client.repository.store_model(model=model, pipeline=pipeline, meta_props=model_props, training_data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to do any cleanup of previously created models and deployments\n",
    "client.repository.list_models()\n",
    "client.deployments.list()\n",
    "\n",
    "# client.repository.delete('GUID of stored model')\n",
    "# client.deployments.delete('GUID of deployed model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Write the test data to a .csv so that we can later use it for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_eval_CSV=test_data.toPandas()\n",
    "write_eval_CSV.to_csv('/project_data/data_asset/HEARTFAILURE-SparkMLEval.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you have created a model based on customer churn data, and deployed it to Watson Machine Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
